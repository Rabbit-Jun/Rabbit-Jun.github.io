---
title: wandb
date: 2024-11-20
category: deeplearning
layout: post
---

# wandb를 사용하자
먼저 wandb에 회원 가입한 후에
`pip install wandb`를 해주고
`wandb login` 후에 홈페이지에 있는 api를 복사해서 넣어주면 된다.
터미널에 입력할 때 키가 보이지 않느데 잉때 너무 당황하지 말자


### Wandb 초기화
```python
wandb.init(project="mlp-binary-classification", name="binary_classification_experiment")
```
### 모델 하이퍼파라미터 로깅
```python
wandb.config.update({
    "epochs": 10,
    "batch_size": 32,
    "learning_rate": 0.001,
    "hidden_size": 64,
    "input_size": train_x.shape[1],
    "output_size": 2
})
```

### Wandb 로그 기록
```python
wandb.log({
    "epoch": epoch + 1,
    "train_loss": train_loss,
    "val_loss": val_loss,
    "train_accuracy": train_accuracy,
    "val_accuracy": val_accuracy
})
```

전체 코드
```python
# Wandb 초기화
wandb.init(project="mlp-binary-classification", name="binary_classification_experiment")

# 모델 하이퍼파라미터 로깅
wandb.config.update({
    "epochs": 10,
    "batch_size": 32,
    "learning_rate": 0.001,
    "hidden_size": 64,
    "input_size": train_x.shape[1],
    "output_size": 2
})

# 훈련 함수 수정
def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        # Training loop
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # Accuracy 계산
            _, predicted = torch.max(outputs, 1)
            correct_train += (predicted == labels).sum().item()
            total_train += labels.size(0)

        train_loss = running_loss / len(train_loader)
        train_accuracy = correct_train / total_train

        # Validation loop
        val_loss = 0.0
        correct_val = 0
        total_val = 0
        model.eval()
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                # Accuracy 계산
                _, predicted = torch.max(outputs, 1)
                correct_val += (predicted == labels).sum().item()
                total_val += labels.size(0)

        val_loss /= len(val_loader)
        val_accuracy = correct_val / total_val

        # 에포크별 출력
        print(f"Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}")
        print(f"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}")

        # Wandb 로그 기록
        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "train_accuracy": train_accuracy,
            "val_accuracy": val_accuracy
        })
```
