---
title: dataload1
date: 2024-11-06
categories: deeplearning
layout: post
---
# dataload
```
(crossfit) ➜  src git:(main) ✗ python train.py
Traceback (most recent call last):
  File "/mnt/c/Users/junun/documents/projects/src/train.py", line 10, in <module>
    train_x, train_y, test_x, test_y, val_x, val_y = load_data('../data/train', '../data/test', '../data/val')
  File "/mnt/c/Users/junun/documents/projects/src/dataload.py", line 52, in load_data
    val_dataset = CustomDataset(val_dir, scaler=scaler)
  File "/mnt/c/Users/junun/documents/projects/src/dataload.py", line 15, in __init__
    self.load_data(directory)
  File "/mnt/c/Users/junun/documents/projects/src/dataload.py", line 35, in load_data
    self.data = self.scaler.transform(self.data)
  File "/home/jun/miniconda3/envs/crossfit/lib/python3.10/site-packages/sklearn/utils/_set_output.py", line 316, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/home/jun/miniconda3/envs/crossfit/lib/python3.10/site-packages/sklearn/preprocessing/_data.py", line 1045, in transform
    X = self._validate_data(
  File "/home/jun/miniconda3/envs/crossfit/lib/python3.10/site-packages/sklearn/base.py", line 654, in _validate_data
    self._check_n_features(X, reset=reset)
  File "/home/jun/miniconda3/envs/crossfit/lib/python3.10/site-packages/sklearn/base.py", line 443, in _check_n_features
    raise ValueError(
ValueError: X has 8904 features, but StandardScaler is expecting 8190 features as input.
```
min() 함수를 이용하여 모든 인덱스를 가장 짧은 인덱스의 길이에 맞추자   
(*샘플의 길이가 다 같아야 한다*)

```python
import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, directory, min_length=None, scaler=None):
        self.data = []
        self.labels = []
        self.min_length = min_length
        self.scaler = scaler
        self.load_data(directory)

    def load_data(self, directory):
        files = [f for f in os.listdir(directory) if f.endswith('.csv')]

        # 각 데이터셋에서 주어진 min_length에 맞춰 자르기
        for filename in files:
            df = pd.read_csv(os.path.join(directory, filename), dtype=np.float32)
            file_data = df.values.flatten()[:self.min_length]
            self.data.append(file_data)
            label = filename.split('@')[0]
            self.labels.append(label)

        self.data = np.vstack(self.data)
        self.labels = np.array(self.labels)

        if self.scaler:
            self.data = self.scaler.transform(self.data)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return torch.tensor(self.data[idx]), torch.tensor(int(self.labels[idx]))

def load_data(train_dir, test_dir, val_dir):
    # 모든 파일에서 최소 길이를 계산
    min_length = float('inf')
    for directory in [train_dir, test_dir, val_dir]:
        files = [f for f in os.listdir(directory) if f.endswith('.csv')]
        for filename in files:
            df = pd.read_csv(os.path.join(directory, filename), dtype=np.float32)
            min_length = min(min_length, df.values.flatten().shape[0])

    scaler = StandardScaler()

    # Training data
    train_dataset = CustomDataset(train_dir, min_length=min_length)
    train_data = scaler.fit_transform(train_dataset.data)
    train_labels = train_dataset.labels

    # Validation data
    val_dataset = CustomDataset(val_dir, min_length=min_length, scaler=scaler)
    val_data = val_dataset.data
    val_labels = val_dataset.labels

    # Test data
    test_dataset = CustomDataset(test_dir, min_length=min_length, scaler=scaler)
    test_data = test_dataset.data
    test_labels = test_dataset.labels

    return train_data, train_labels, test_data, test_labels, val_data, val_labels
def load_data(train_dir, test_dir, val_dir):
    scaler = StandardScaler()

    # Training data
    train_dataset = CustomDataset(train_dir)
    train_data = scaler.fit_transform(train_dataset.data)
    train_labels = train_dataset.labels

    # Validation data
    val_dataset = CustomDataset(val_dir, scaler=scaler)
    val_data = val_dataset.data
    val_labels = val_dataset.labels

    # Test data
    test_dataset = CustomDataset(test_dir, scaler=scaler)
    test_data = test_dataset.data
    test_labels = test_dataset.labels

    return train_data, train_labels, test_data, test_labels, val_data, val_labels

```


```
Traceback (most recent call last):
  File "/mnt/c/Users/junun/documents/projects/src/train.py", line 10, in <module>
    train_x, train_y, test_x, test_y, val_x, val_y = load_data('../data/train', '../data/test', '../data/val')
  File "/mnt/c/Users/junun/documents/projects/src/dataload.py", line 70, in load_data
    train_dataset = CustomDataset(train_dir)
  File "/mnt/c/Users/junun/documents/projects/src/dataload.py", line 14, in __init__
    self.load_data(directory)
  File "/mnt/c/Users/junun/documents/projects/src/dataload.py", line 27, in load_data
    self.data = np.array(self.data)
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1778,) + inhomogeneous part.
```
StandardScaler로 표준화를 하기 위해서는 2차원 배열이 필요하다

```python
        # numpy 배열로 변환하여 2차원 배열로 유지
        self.data = np.vstack(self.data)
        self.labels = np.array(self.labels)

```


self.data.reshape(-1, 1)을 사용하면 원하는 결과를 얻지 못할 가능성이 높습니다.  
reshape(-1, 1)은 배열을 모든 요소를 단일 열로 펼쳐서 재구성하기 때문에, 각 샘플의 고유한 구조가 손실됩니다.  

예를 들어, self.data가 여러 샘플이 들어 있는 리스트라면 reshape(-1, 1)을 사용하면 각 샘플이 하나의 긴 열로 병합되어, 샘플 간 경계가 사라지게 됩니다.  
이는 우리가 원하는 2차원 구조 (즉, (샘플 수, 피처 수))와는 다르기 때문에 StandardScaler가 기대하는 입력 형태를 만족하지 않습니다.  

**대안: np.vstack을 사용하는 이유**
np.vstack(self.data)는 self.data에 있는 각 샘플을 행렬의 행으로 쌓아 2차원 구조를 유지하게 해줍니다. 이렇게 하면 (샘플 수, 피처 수)의 형태가 되어 StandardScaler가 예상하는 입력 형태와 일치하게 됩니다.  


```
ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 11046 and the array at index 8 has size 11088
```

```python
    # 모든 파일에서 최소 길이를 계산
    min_length = float('inf')
    for directory in [train_dir, test_dir, val_dir]:
        files = [f for f in os.listdir(directory) if f.endswith('.csv')]
        for filename in files:
            df = pd.read_csv(os.path.join(directory, filename), dtype=np.float32)
            min_length = min(min_length, df.values.flatten().shape[0])
    print(f"Calculated min_length: {min_length}")
```
뭐가 문제인지 출력해서 확인해 보기로  
확인해 보니 아예 함수가 호출이 안된다???  

문제는 클래스의 메서드와 함수가 load_data로 같은 이름을 사용해서 그런것  
바로 메서드의 이름을과 __init__의 load_data ->load_files로 변경  

```
Traceback (most recent call last):
  File "/mnt/c/Users/junun/documents/projects/src/train.py", line 13, in <module>
    train_dataset = TensorDataset(torch.tensor(train_x, dtype=torch.float32), torch.tensor(train_y, dtype=torch.long))
TypeError: can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.
```
이 오류는 numpy.ndarray에 문자열 데이터(numpy.str_)가 포함되어 있을 때 발생합니다.  
즉, 라벨에 값을 문자로 넣으면 안되고 숫자 값으로 바꿔줘야 한다는 의미

원핫 인코딩을 하기로 결정!!  

```
Traceback (most recent call last):
  File "/mnt/c/Users/junun/documents/projects/src/train.py", line 10, in <module>
    train_x, train_y, test_x, test_y, val_x, val_y = load_data('../data/train', '../data/test', '../data/val')
  File "/mnt/c/Users/junun/documents/projects/src/dataload.py", line 62, in load_data
    label_encoder.fit(train_dataset.labels)
UnboundLocalError: local variable 'train_dataset' referenced before assignment
```
개같이 멸망!!!  
이 오류는 train_dataset이 label_encoder.fit() 호출 전에 정의되지 않았기 때문에 발생합니다.  
train_dataset을 먼저 생성한 후, label_encoder로 레이블을 학습해야 합니다.  