---
title: "mnist"
layout: post
date: 2024-04-12
categories: hnv
---
# ipynb에서 GPU 메모리 지우기
```python
import IPython
app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```
# 트레이닝 및 검증 데이터 및 레이블
- x_train: 뉴럴 네트워크를 트레이닝하는 데 사용되는 이미지
- y_train: x_train 트레이닝 중 모델의 예측을 평가하는 데 사용되는 올바른 이미지 레이블
- x_valid: 트레이닝된 모델의 성능 검증을 위해 따로 확보해 놓는 이미지
- y_valid: x_valid 트레이닝 후 모델의 예측을 평가하는 데 사용되는 올바른 이미지 레이블

# 데이터를 메모리에 로드(Keras 사용)

```python
from tensorflow.keras.datasets import mnist
# the data, split between train and validation sets
(x_train, y_train), (x_valid, y_valid) = mnist.load_data()
```

# MNIST 데이터 살펴보기

```python
x_train.shape
x_valid.shape
x_train.dtype
x_train.min()
x_train.max()
x_train[0]
```
<pre>
MNIST 데이터세트에는 수기 문자로 이루어진 70,000개의 회색조 이미지가 포함되어 있습니다. 다음 셀을 실행하면 Keras가 트레이닝을 위해 60,000개의 이미지, 검증(트레이닝 후)을 위해 10,000개의 이미지를 분할했으며 각 이미지 자체가 28x28 차원의 2D 어레이임을 확인할 수 있습니다.
뿐만 아니라, 이러한 28x28 이미지가 0~255의 서명되지 않은 8비트 정수 값 모음으로 표현되는 것을 확인할 수 있습니다. 이는 픽셀의 회색조 값에 해당하는 값들로, 0은 검은색, 255는 흰색, 그리고 나머지 모든 값은 둘 사이의 값에 해당합니다.
</pre>

- Matplotlib을 사용하면 데이터세트에서 이러한 회색조 이미지 중 하나를 렌더링할 수 있다.

```python
import matplotlib.pyplot as plt

image = x_train[0]
plt.imshow(image, cmap='gray')
```

- 답은 데이터의 올바른 레이블을 포함하는 y_train 데이터에 있습니다. 

```python
y_train[0]
```

# 트레이닝을 위한 데이터 준비

1. 이미지 데이터를 평탄화하여 모델에 입력되는 이미지를 간소화해야 합니다.
2. 이미지 데이터를 정규화하여 이미지 입력 값이 모델에서 더 쉽게 작동되도록 해야 합니다.
3. 레이블을 분류하여 레이블 값이 모델에서 더 쉽게 작동되도록 해야 합니다.

# 이미지 데이터 평탄화

- 딥러닝 모델에서 2차원 이미지(우리 경우에는 28x28픽셀)를 사용할 수도 있긴 하지만 여기서는 간단하게 각 이미지를 784개의 연속 픽셀(참고: 28x28 = 784)로 이루어진 단일 어레이로 재구성(reshape)하겠습니다. 
- 이는 이미지 평탄화라고도 불립니다.

```python
x_train = x_train.reshape(60000, 784)
x_valid = x_valid.reshape(10000, 784)
```

```python
x_train.shape
x_train[0]
```

-이미지가 재구성되어 각 784개의 픽셀 값을 포함하는 1D 어레이의 모음이 되었음을 확인할 수 있습니다.

# 이미지 데이터 정규화
- 딥러닝 모델은 0에서 1 사이의 부동 소수점 수를 처리하는 데 더 뛰어납니다
- 정수 값을 0에서 1 사이의 부동 소수점 값으로 변환하는 것을 **정규화**라고 한다
-  여기서는 데이터를 정규화하기 위해 모든 픽셀 값(앞에서 언급했던 것처럼 0~255)을 255로 나누는 단순한 접근 방법을 사용하겠습니다.

```python
x_train = x_train / 255
x_valid = x_valid / 255 
```

```python
x_train.dtype
x_train.min()
x_train.max()
```

# 범주 인코딩
- 7-2의 답이 뭐냐고 묻는 질문을 받는다고 가정해 보겠습니다
    -  4라고 답하는 것은 9라고 답하는 것보다 훨씬 정답에 근접하지만 이미지 분류 문제의 경유 뉴럴 네트워크가 이런 종류의 추론을 배우는 것은 안 좋다. (*4를 추측하는 것이 9를 추측하는 것 만큼이나 나쁘다*)

- 이미지의 레이블은 현재 상태 그대로 0~9의 정수입니다. 이러한 값이 숫자 범위를 나타내므로, 모델은 올바른 숫자 범주에 얼마나 근접하게 추측하는지에 따라 성능에 대한 어떤 결론을 내리려 할 수도 있습니다.
- 따라서 여기서는 데이터에 범주 인코딩이라는 작업을 수행하겠습니다. 이러한 변환은 이 특정 값이 true로 설정된 실제 범주를 포함해 각 값이 가능한 모든 범주의 모음이 되도록 데이터를 수정합니다.
- ex)
|Actual Color| Is Red? | Is Blue? | Is Green?|
|------------|---------|----------|----------|
|Red|True|False|False|
|Green|False|False|True|
|Blue|False|True|False|
|Green|False|False|True|

|Actual Color| Is Red? | Is Blue? | Is Green?|
|------------|---------|----------|----------|
|Red|1|0|0|
|Green|0|0|1|
|Blue|0|1|0|
|Green|0|0|1|

- 이것이 범주 인코딩 즉, 범주 레이블로 이해되어야 하는 값을 모델이 범주 특성을 알 수 있는 표현으로 변환하는 것입니다. 따라서 다음의 값을 트레이닝에 사용할 경우

```python
values = ['red, green, blue, green'] # 이렇게 하면 뉴럴 네트워크가 이해하기 힘듬

values = [
    [1, 0, 0], # red
    [0, 0, 1], # green
    [0, 1, 0], # blue
    [0, 0, 1]  # green
] # 이렇게 변경
```

# 레이블 범주 인코딩
- Keras는 값을 범주 인코딩하는 유틸리티를 제공하며, 여기서는 이를 사용하여 트레이닝 및 검증 레이블 모두에 대한 범주 인코딩을 수행합니다.

```python
import tensorflow.keras as keras
num_categories = 10

y_train = keras.utils.to_categorical(y_train, num_categories)
y_valid = keras.utils.to_categorical(y_valid, num_categories)
```

```python
y_train[0:9] #  트레이닝 레이블과 관련된 첫 번째 10개 값으로, 지금은 범주 인코딩이 되어 있는 것을 확인
```

# 모델 생성
1. 어느 정도 예상되는 형식으로 데이터를 수신하는 입력 레이어
2. 각각 다수의 뉴런으로 구성된 여러 개의 숨겨진 레이어 각 뉴런은 가중치로 네트워크의 추측에 영향을 미칠 수 있으며, 가중치는 네트워크가 수많은 반복을 통해 성능에 대한 피드백을 수신하고 학습하면서 업데이트하게 되는 값입니다.
3. 주어진 이미지에 대한 네트워크의 추측을 보여주는 출력 레이어

# 모델 인스턴스화
- 우선 Keras의 순차 모델 클래스를 사용하여 데이터가 연속으로 통과할 일련의 레이어를 보유한 모델의 인스턴스를 인스턴스화

```python
from tensorflow.keras.models import Sequential

model = Sequential()
```

# 입력 레이어 생성
- , 입력 레이어를 추가합니다. 이 레이어는 밀집 연결되어 있습니다. 따라서 포함된 각 뉴런과 가중치가 다음 레이어의 모든 뉴런에 영향을 줍니다. Keras로 이를 수행하려면 Keras의 Dense 레이어 클래스를 사용

```python
from tensorflow.keras.layers import Dense
```

-units 인수는 레이어 내 뉴런 수를 지정합니다. 여기서는 실험에서 선택한 512를 사용하겠습니다. 올바른 뉴런 수를 선택하는 것은 데이터세트의 통계적 복잡성을 없애주는 일이므로, "데이터 사이언스" 작업의 핵심이라 할 수 있습니다
-input_shape 값은 수신되는 데이터의 모양을 지정하며, 여기서는 784개 값으로 이루어진 1D 어레이

```python
model.add(Dense(units=512, activation='relu', input_shape=(784,)))
```

# 숨겨진 레이어 생성
- 지금은 이러한 레이어가 추측에 기여하는 더 많은 매개변수 즉, 정확한 학습을 위한 좀 더 예리한 기회를 네트워크에 제공한다는 사실을 알면 됩니다.

```python
model.add(Dense(units = 512, activation='relu'))
```

# 출력 레이어 생성
-  레이어는 각 레이어의 값이 0에서 1사이의 확률이 되도록 하고 레이어의 모든 출력이 1에 추가되도록 하는 활성 함수인 softmax를 사용
- 이 경우에는 네트워크가 1에서 10까지의 가능한 범주에 속하는 단일 이미지에 대해 추측을 수행하므로 출력은 10개가 됩니다.
- 각 출력은 이미지가 해당 특정 클래스에 속한다는 모델의 추측(확률)을 제공합니다.

```python
model.add(Dense(units = 10, activation='softmax'))
```
# 모델 요약
- Keras는 모델에 대한 읽을 수 있는 요약을 출력하는 모델 인스턴스 메서드 요약을 제공

```python
model.summary()
```

- 트레이닝 가능한 매개변수의 수를 확인하십시오. 이러한 각 매개변수는 트레이닝 도중에 조정 가능하며 트레이닝된 모델의 추측에 기여합니다.

# 모델 컴파일
- 트레이닝 중 모델에서 성능을 파악하는 데 사용되는 손실 함수를 지정합니다. 또한 모델 트레이닝 동안 accuracy도 추적하도록 지정합니다.

```python
model.compile(loss='categorical_crossentropy', metrics=['accuracy'])
```

# 모델 트레이닝
- 트레이닝 및 검증 데이터와 모델이 준비되었으니 이제 트레이닝 데이터로 모델을 트레이닝하고 검증 데이터로 이를 검증해야 합니다.
- 데이터로 모델을 트레이닝"하는 것을 흔히 "모델을 데이터에 맞춘다"라고도 합니다. 모델을 데이터에 맞춘다는 말은 주어지고 있는 데이터를 좀 더 정확하게 이해하기 위해 모델이 점차적으로 모양을 바꾼다는 점을 부각시킵니다

- Keras로 모델을 맞추는(트레이닝하는) 경우에는 모델의 fit 메서드를 사용합니다. 그러면 다음 인수를 예상합니다.

<pre>
트레이닝 데이터
트레이닝 데이터의 레이블
전체 트레이닝 데이터세트에 대해 트레이닝해야 하는 횟수(에포크)
검증 또는 테스트 데이터 및 해당 레이블
트레이닝 데이터
트레이닝 데이터의 레이블
전체 트레이닝 데이터세트에 대해 트레이닝해야 하는 횟수(에포크)
검증 또는 테스트 데이터 및 해당 레이블
</pre>

```python
history = model.fit(
    x_train, y_train, epochs=5, verbose=1, validation_data=(x_valid, y_valid)
)
```

# 정확도 관찰
- 5회의 에포크 각각에 대해 accuracy 및 val_accuracy 점수를 살펴보십시오. accuracy는 모든 트레이닝 데이터에 대한 에포크 동안의 모델 성능이 어땠는지를 명시합니다.
- val_accuracy는 모델을 트레이닝하는 데 전혀 사용되지 않는 검증 데이터에 대한 모델 성능이 어땠는지를 나타냅니다.
- 새로운 근사한 머신 러닝 아키텍처를 작동하는 데 어려움을 겪고 계십니까? MNIST와 비교해 보십시오. 이 데이터세트에 대해 학습할 수 없다면 더 복잡한 이미지와 데이터세트에 대해 학습할 수 없을 가능성이 높습니다.
